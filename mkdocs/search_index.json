{
    "docs": [
        {
            "location": "/",
            "text": "Applied Machine Learning\n\n\nIntroduction\n\n\nPipeline\n\n\nDemo links",
            "title": "Home"
        },
        {
            "location": "/#applied-machine-learning",
            "text": "",
            "title": "Applied Machine Learning"
        },
        {
            "location": "/#introduction",
            "text": "",
            "title": "Introduction"
        },
        {
            "location": "/#pipeline",
            "text": "",
            "title": "Pipeline"
        },
        {
            "location": "/#demo-links",
            "text": "",
            "title": "Demo links"
        },
        {
            "location": "/docker/",
            "text": "Docker\n\n\nStep by step\n\n\nClone repository\n\n\ngit clone https://github.com/amirbawab/corpus-tools\n\n\n\n\nBuild docker image\n\n\nsudo docker build -t reddit:latest .\n\n\n\n\nStart instance\n\n\nsudo docker start --name reddit-container -it reddit:latest\n\n\n\n\nStart pipeline\n\n\nFrom container\n\n\n./reddit.sh\n# Use the file: french_comments_part.json\n\n\n\n\nNote: On the google cloud, you can use the large file",
            "title": "Docker"
        },
        {
            "location": "/docker/#docker",
            "text": "",
            "title": "Docker"
        },
        {
            "location": "/docker/#step-by-step",
            "text": "",
            "title": "Step by step"
        },
        {
            "location": "/docker/#clone-repository",
            "text": "git clone https://github.com/amirbawab/corpus-tools",
            "title": "Clone repository"
        },
        {
            "location": "/docker/#build-docker-image",
            "text": "sudo docker build -t reddit:latest .",
            "title": "Build docker image"
        },
        {
            "location": "/docker/#start-instance",
            "text": "sudo docker start --name reddit-container -it reddit:latest",
            "title": "Start instance"
        },
        {
            "location": "/docker/#start-pipeline",
            "text": "From container  ./reddit.sh\n# Use the file: french_comments_part.json  Note: On the google cloud, you can use the large file",
            "title": "Start pipeline"
        },
        {
            "location": "/ngram/",
            "text": "Ngram\n\n\nIntroduction\n\n\nNgram is a command line based program which takes a list of conversations and a word as input, and outputs\nthe word that could possible follow the provided one. The current release of the this tool builds an Ngram \ntable where N = 2, also called Bigram.\n\n\nBuild the tool\n\n\n\n\nClone repository\n\n\nClone submodules \ngit clone submodule update --init --recursive\n\n\nGo the tool directory \ncd tools/ngram/\n\n\nBuild the tool \ncmake . && make\n\n\n\n\nHow to use Ngram tool?\n\n\nNgram provides output on request. The information displayed on the console can be controlled using the flags\nand arguments that are passed to the program. In the output, the strings \n<s>\n and \n</s>\n denote the \nbeginning and the end of a sentence.\n\n\nNote: Use the \n-h\n or \n--help\n flags to learn more about the latest functionalities of the tool.\n\n\nHow does it work?\n\n\nThe Ngram tool stores a Bigram table and a Word occurence vector. The two latter are used to guess what are\nthe possible words that could follow the provided one.\n\n\nBigram table\n\n\nThe Bigram table is a lookup table that keeps track of the number of times the word W\ni\n occurres \nbefore W\ni+1\n.\n\n\nWord occurence\n\n\nThe Word occurence vector keeps track of how many times a word occurred in the provided conversations.\n\n\nGuess the next word\n\n\nUsing the Bigram table and the word occurence vector, W\ni+1\n can be guessed:\n\nP(W\ni+1\n | W\ni\n) = Count(W\ni\n, W\ni+1\n) / Count(W\ni\n)\n\n\nNote: The program will print all the possible words sorted by their probabilities.\n\n\nExamples\n\n\nExample 1\n\n\n$ cat dataset.xml | ./ngram -i -- -c -w \"<s>\" -T 0.2\nNgram table\n-----------\ncount(<s>, Hey) = 1\ncount(<s>, I\u2019m) = 1\ncount(<s>, Me) = 2\ncount(<s>, Nice) = 1\ncount(<s>, Who\u2019s) = 1\ncount(Hey, how) = 1\ncount(I\u2019m, fine) = 1\ncount(Me, </s>) = 1\ncount(Me, too) = 1\ncount(Nice, </s>) = 1\ncount(Who\u2019s, around) = 1\ncount(are, you) = 1\ncount(around, for) = 1\ncount(fine, thank) = 1\ncount(for, lunch) = 1\ncount(how, are) = 1\ncount(lunch, </s>) = 1\ncount(thank, you) = 1\ncount(too, </s>) = 1\ncount(you, </s>) = 2\n---\n\nNgram word occurrence\n---------------------\ncount(</s>) = 6\ncount(<s>) = 6\ncount(Hey) = 1\ncount(I\u2019m) = 1\ncount(Me) = 2\ncount(Nice) = 1\ncount(Who\u2019s) = 1\ncount(are) = 1\ncount(around) = 1\ncount(fine) = 1\ncount(for) = 1\ncount(how) = 1\ncount(lunch) = 1\ncount(thank) = 1\ncount(too) = 1\ncount(you) = 2\n---\n\nPossible word(s) after <s>\n---------------------\nP(Me | <s>) = 0.333333\n\n\n\n\nExample 2\n\n\n$ cat dataset.xml | ./ngram -i -- -c -w \"hey\" -I\nNgram table\n-----------\ncount(<s>, hey) = 1\ncount(<s>, i\u2019m) = 1\ncount(<s>, me) = 2\ncount(<s>, nice) = 1\ncount(<s>, who\u2019s) = 1\ncount(are, you) = 1\ncount(around, for) = 1\ncount(fine, thank) = 1\ncount(for, lunch) = 1\ncount(hey, how) = 1\ncount(how, are) = 1\ncount(i\u2019m, fine) = 1\ncount(lunch, </s>) = 1\ncount(me, </s>) = 1\ncount(me, too) = 1\ncount(nice, </s>) = 1\ncount(thank, you) = 1\ncount(too, </s>) = 1\ncount(who\u2019s, around) = 1\ncount(you, </s>) = 2\n---\n\nNgram word occurrence\n---------------------\ncount(</s>) = 6\ncount(<s>) = 6\ncount(are) = 1\ncount(around) = 1\ncount(fine) = 1\ncount(for) = 1\ncount(hey) = 1\ncount(how) = 1\ncount(i\u2019m) = 1\ncount(lunch) = 1\ncount(me) = 2\ncount(nice) = 1\ncount(thank) = 1\ncount(too) = 1\ncount(who\u2019s) = 1\ncount(you) = 2\n---\n\nPossible word(s) after hey\n---------------------\nP(how | hey) = 1",
            "title": "Ngram"
        },
        {
            "location": "/ngram/#ngram",
            "text": "",
            "title": "Ngram"
        },
        {
            "location": "/ngram/#introduction",
            "text": "Ngram is a command line based program which takes a list of conversations and a word as input, and outputs\nthe word that could possible follow the provided one. The current release of the this tool builds an Ngram \ntable where N = 2, also called Bigram.",
            "title": "Introduction"
        },
        {
            "location": "/ngram/#build-the-tool",
            "text": "Clone repository  Clone submodules  git clone submodule update --init --recursive  Go the tool directory  cd tools/ngram/  Build the tool  cmake . && make",
            "title": "Build the tool"
        },
        {
            "location": "/ngram/#how-to-use-ngram-tool",
            "text": "Ngram provides output on request. The information displayed on the console can be controlled using the flags\nand arguments that are passed to the program. In the output, the strings  <s>  and  </s>  denote the \nbeginning and the end of a sentence.  Note: Use the  -h  or  --help  flags to learn more about the latest functionalities of the tool.",
            "title": "How to use Ngram tool?"
        },
        {
            "location": "/ngram/#how-does-it-work",
            "text": "The Ngram tool stores a Bigram table and a Word occurence vector. The two latter are used to guess what are\nthe possible words that could follow the provided one.",
            "title": "How does it work?"
        },
        {
            "location": "/ngram/#bigram-table",
            "text": "The Bigram table is a lookup table that keeps track of the number of times the word W i  occurres \nbefore W i+1 .",
            "title": "Bigram table"
        },
        {
            "location": "/ngram/#word-occurence",
            "text": "The Word occurence vector keeps track of how many times a word occurred in the provided conversations.",
            "title": "Word occurence"
        },
        {
            "location": "/ngram/#guess-the-next-word",
            "text": "Using the Bigram table and the word occurence vector, W i+1  can be guessed: \nP(W i+1  | W i ) = Count(W i , W i+1 ) / Count(W i )  Note: The program will print all the possible words sorted by their probabilities.",
            "title": "Guess the next word"
        },
        {
            "location": "/ngram/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/ngram/#example-1",
            "text": "$ cat dataset.xml | ./ngram -i -- -c -w \"<s>\" -T 0.2\nNgram table\n-----------\ncount(<s>, Hey) = 1\ncount(<s>, I\u2019m) = 1\ncount(<s>, Me) = 2\ncount(<s>, Nice) = 1\ncount(<s>, Who\u2019s) = 1\ncount(Hey, how) = 1\ncount(I\u2019m, fine) = 1\ncount(Me, </s>) = 1\ncount(Me, too) = 1\ncount(Nice, </s>) = 1\ncount(Who\u2019s, around) = 1\ncount(are, you) = 1\ncount(around, for) = 1\ncount(fine, thank) = 1\ncount(for, lunch) = 1\ncount(how, are) = 1\ncount(lunch, </s>) = 1\ncount(thank, you) = 1\ncount(too, </s>) = 1\ncount(you, </s>) = 2\n---\n\nNgram word occurrence\n---------------------\ncount(</s>) = 6\ncount(<s>) = 6\ncount(Hey) = 1\ncount(I\u2019m) = 1\ncount(Me) = 2\ncount(Nice) = 1\ncount(Who\u2019s) = 1\ncount(are) = 1\ncount(around) = 1\ncount(fine) = 1\ncount(for) = 1\ncount(how) = 1\ncount(lunch) = 1\ncount(thank) = 1\ncount(too) = 1\ncount(you) = 2\n---\n\nPossible word(s) after <s>\n---------------------\nP(Me | <s>) = 0.333333",
            "title": "Example 1"
        },
        {
            "location": "/ngram/#example-2",
            "text": "$ cat dataset.xml | ./ngram -i -- -c -w \"hey\" -I\nNgram table\n-----------\ncount(<s>, hey) = 1\ncount(<s>, i\u2019m) = 1\ncount(<s>, me) = 2\ncount(<s>, nice) = 1\ncount(<s>, who\u2019s) = 1\ncount(are, you) = 1\ncount(around, for) = 1\ncount(fine, thank) = 1\ncount(for, lunch) = 1\ncount(hey, how) = 1\ncount(how, are) = 1\ncount(i\u2019m, fine) = 1\ncount(lunch, </s>) = 1\ncount(me, </s>) = 1\ncount(me, too) = 1\ncount(nice, </s>) = 1\ncount(thank, you) = 1\ncount(too, </s>) = 1\ncount(who\u2019s, around) = 1\ncount(you, </s>) = 2\n---\n\nNgram word occurrence\n---------------------\ncount(</s>) = 6\ncount(<s>) = 6\ncount(are) = 1\ncount(around) = 1\ncount(fine) = 1\ncount(for) = 1\ncount(hey) = 1\ncount(how) = 1\ncount(i\u2019m) = 1\ncount(lunch) = 1\ncount(me) = 2\ncount(nice) = 1\ncount(thank) = 1\ncount(too) = 1\ncount(who\u2019s) = 1\ncount(you) = 2\n---\n\nPossible word(s) after hey\n---------------------\nP(how | hey) = 1",
            "title": "Example 2"
        },
        {
            "location": "/tex/Readme/",
            "text": "Tex documentation\n\n\nCompile to PDF\n\n\npdflatex uml.tex",
            "title": "Readme"
        },
        {
            "location": "/tex/Readme/#tex-documentation",
            "text": "",
            "title": "Tex documentation"
        },
        {
            "location": "/tex/Readme/#compile-to-pdf",
            "text": "pdflatex uml.tex",
            "title": "Compile to PDF"
        }
    ]
}